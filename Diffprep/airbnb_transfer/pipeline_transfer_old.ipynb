{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddharth/miniconda3/envs/diffprep/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "from experiment.experiment_utils import set_random_seed, load_data, build_data, grid_search, makedir, save_result, load_data_multitask\n",
    "from model import LogisticRegression\n",
    "from pipeline.diffprep_flex_pipeline import DiffPrepFlexPipeline\n",
    "from pipeline.diffprep_fix_pipeline import DiffPrepFixPipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from trainer.diffprep_trainer import DiffPrepSGD\n",
    "from utils import SummaryWriter\n",
    "from experiment.experiment_utils import min_max_normalize\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prep_pipeline(path, prep_space, params, data_dir, dataset):\n",
    "\n",
    "    X, y = load_data_multitask(data_dir, dataset)\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = build_data(X, y, random_state=params[\"split_seed\"])\n",
    "\n",
    "    prep_pipeline = DiffPrepFixPipeline(prep_space, temperature=params[\"temperature\"],\n",
    "                                use_sample=False,\n",
    "                                diff_method=params[\"diff_method\"],\n",
    "                                init_method=params[\"init_method\"])\n",
    "    prep_pipeline.init_parameters(X_train, X_val, X_test)\n",
    "    prep_pipeline.load_state_dict(torch.load(path))\n",
    "    prep_pipeline.is_fitted = True\n",
    "    prep_pipeline.fit(X_train)\n",
    "    #prep_pipeline.eval()\n",
    "\n",
    "    return prep_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffPrepExperiment(object):\n",
    "    \"\"\"Run auto prep with one set of hyper parameters\"\"\"\n",
    "    def __init__(self, data_dir, dataset, prep_space, model_name, method, fixed_pipeline_path=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset = dataset\n",
    "        self.prep_space = prep_space\n",
    "        self.model_name = model_name\n",
    "        self.method = method\n",
    "        self.fixed_pipeline_path = fixed_pipeline_path\n",
    "\n",
    "    def run(self, params, verbose=True):        \n",
    "        X, y = load_data_multitask(self.data_dir, self.dataset)\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = build_data(X, y, random_state=params[\"split_seed\"])\n",
    "        \n",
    "        print(\"Dataset shapes: \", X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "        # pre norm for diffprep flex\n",
    "        if self.method == \"diffprep_flex\":\n",
    "            X_train, X_val, X_test = min_max_normalize(X_train, X_val, X_test)\n",
    "            params[\"patience\"] = 10\n",
    "            params[\"num_epochs\"] = 3000\n",
    "\n",
    "        # set random seed\n",
    "        set_random_seed(params)\n",
    "        ## transform pipeline\n",
    "        # define and fit first step\n",
    "        if self.fixed_pipeline_path:\n",
    "            #prep_pipeline = self.fixed_pipeline\n",
    "            prep_pipeline = DiffPrepFixPipeline(self.prep_space, temperature=params[\"temperature\"],\n",
    "                                use_sample=False,\n",
    "                                diff_method=params[\"diff_method\"],\n",
    "                                init_method=params[\"init_method\"])\n",
    "            prep_pipeline.init_parameters(X_train, X_val, X_test)\n",
    "            prep_pipeline.load_state_dict(torch.load(self.fixed_pipeline_path))\n",
    "            prep_pipeline.fit(X_train)\n",
    "            prep_pipeline.is_fitted = True\n",
    "\n",
    "        elif self.method == \"diffprep_fix\":\n",
    "            prep_pipeline = DiffPrepFixPipeline(self.prep_space, temperature=params[\"temperature\"],\n",
    "                                             use_sample=params[\"sample\"],\n",
    "                                             diff_method=params[\"diff_method\"],\n",
    "                                             init_method=params[\"init_method\"])\n",
    "            prep_pipeline.init_parameters(X_train, X_val, X_test)\n",
    "        elif self.method == \"diffprep_flex\":\n",
    "            prep_pipeline = DiffPrepFlexPipeline(self.prep_space, temperature=params[\"temperature\"],\n",
    "                            use_sample=params[\"sample\"],\n",
    "                            diff_method=params[\"diff_method\"],\n",
    "                            init_method=params[\"init_method\"])\n",
    "            prep_pipeline.init_parameters(X_train, X_val, X_test)\n",
    "        else:\n",
    "            raise Exception(\"Wrong auto prep method\")\n",
    "\n",
    "        #prep_pipeline.init_parameters(X_train, X_val, X_test)\n",
    "        print(\"Train size: ({}, {})\".format(X_train.shape[0], prep_pipeline.out_features))\n",
    "\n",
    "        # model\n",
    "        input_dim = prep_pipeline.out_features\n",
    "        output_dim = len(set(y.values.ravel()))\n",
    "\n",
    "        # model = TwoLayerNet(input_dim, output_dim)\n",
    "        set_random_seed(params)\n",
    "        if self.model_name == \"log\":\n",
    "            model = LogisticRegression(input_dim, output_dim)\n",
    "        else:\n",
    "            raise Exception(\"Wrong model\")\n",
    "\n",
    "        model = model.to(params[\"device\"])\n",
    "\n",
    "        # loss\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # optimizer\n",
    "        model_optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=params[\"model_lr\"],\n",
    "            weight_decay=params[\"weight_decay\"],\n",
    "            momentum=params[\"momentum\"]\n",
    "        )\n",
    "        \n",
    "        if params[\"prep_lr\"] is None:\n",
    "            prep_lr = params[\"model_lr\"]\n",
    "        else:\n",
    "            prep_lr = params[\"prep_lr\"]\n",
    "    \n",
    "        prep_pipeline_optimizer = None\n",
    "        # torch.optim.Adam(\n",
    "        #     prep_pipeline.parameters(),\n",
    "        #     lr=prep_lr,\n",
    "        #     betas=(0.5, 0.999),\n",
    "        #     weight_decay=params[\"weight_decay\"]\n",
    "        # )\n",
    "\n",
    "        # scheduler\n",
    "        # model_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, patience=patience, factor=0.1, threshold=0.001)\n",
    "        prep_pipeline_scheduler = None\n",
    "        model_scheduler = None\n",
    "\n",
    "        if params[\"logging\"]:\n",
    "            logger = SummaryWriter()\n",
    "        else:\n",
    "            logger = None\n",
    "\n",
    "        diff_prep = DiffPrepSGD(prep_pipeline, model, loss_fn, model_optimizer, prep_pipeline_optimizer,\n",
    "                    model_scheduler, prep_pipeline_scheduler, params, writer=logger, train_pipeline=False)\n",
    "\n",
    "        result, best_model = diff_prep.fit(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "        return result, best_model, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diffprep(data_dir, dataset, result_dir, prep_space, params, model_name, method, prep_pipeline_price_path):\n",
    "    print(\"Dataset:\", dataset, \"Diff Method:\", params[\"diff_method\"], method)\n",
    "\n",
    "    sample = \"sample\" if params[\"sample\"] else \"nosample\"\n",
    "    diff_prep_exp = DiffPrepExperiment(data_dir, dataset, prep_space, model_name, method, fixed_pipeline_path=prep_pipeline_price_path)\n",
    "    best_result, best_model, best_logger, best_params = grid_search(diff_prep_exp, deepcopy(params))\n",
    "    save_result(best_result, best_model, best_logger, best_params, result_dir, save_model=True)\n",
    "    print(\"DiffPrep Finished. val acc:\", best_result[\"best_val_acc\"], \"test acc\", best_result[\"best_test_acc\"])\n",
    "    return best_result, best_model, best_logger, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run diffprep_fix on dataset Airbnb\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from prep_space import space\n",
    "from experiment.baseline_experiment import run_baseline\n",
    "import os\n",
    "\n",
    "# define hyper parameters\n",
    "params = {\n",
    "    \"num_epochs\": 2000,\n",
    "    \"batch_size\": 512,\n",
    "    \"device\": \"cpu\",\n",
    "    #\"model_lr\": [0.1, 0.01, 0.001],\n",
    "    \"model_lr\": 0.01,\n",
    "    \"weight_decay\": 0,\n",
    "    \"model\": 'log',\n",
    "    \"train_seed\": 1,\n",
    "    \"split_seed\": 1,\n",
    "    \"method\": \"diffprep_fix\",\n",
    "    \"save_model\": True,\n",
    "    \"logging\": False,\n",
    "    \"no_crash\": False,\n",
    "    \"patience\": 3,\n",
    "    \"momentum\": 0.9\n",
    "}\n",
    "\n",
    "auto_prep_params = {\n",
    "    \"prep_lr\": None,\n",
    "    \"temperature\": 0.1,\n",
    "    \"grad_clip\": None,\n",
    "    \"pipeline_update_sample_size\": 512,\n",
    "    \"init_method\": \"default\",\n",
    "    \"diff_method\": \"num_diff\",\n",
    "    \"sample\": False\n",
    "}\n",
    "\n",
    "DATADIR = \"data\"\n",
    "\n",
    "params.update(auto_prep_params)\n",
    "\n",
    "datasets = sorted(os.listdir(DATADIR))\n",
    "dataset = \"Airbnb\"\n",
    "\n",
    "print(\"Run {} on dataset {}\".format(params[\"method\"], dataset))\n",
    "\n",
    "result_dir = utils.makedir([\"result\", params[\"method\"], dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Airbnb Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (600, 38) torch.Size([600]) (200, 38) torch.Size([200]) (200, 38) torch.Size([200])\n",
      "Train size: (600, 94)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [00:13<00:54, 29.51it/s, next_eval_time=3s, tr_loss=0.545, val_loss=0.764]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.69 test acc 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prep_pipeline_price_path = './result/diffprep_fix/Airbnb/Price/prep_pipeline.pth'\n",
    "\n",
    "if params[\"method\"] in [\"diffprep_fix\", \"diffprep_flex\"]:\n",
    "    best_result, best_model, best_logger, best_params = run_diffprep(DATADIR, dataset, result_dir, space, params, params[\"model\"], params[\"method\"], prep_pipeline_price_path)\n",
    "else:\n",
    "    best_result, best_model, best_logger, best_params = run_baseline(DATADIR, dataset, result_dir, space, params, params[\"model\"], params[\"method\"], prep_pipeline_price_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('pipeline.0.num_tf_prob_logits',\n",
       "              tensor([[-0.5294, -2.2761, -2.2764, -1.9794, -2.2728],\n",
       "                      [-0.7023, -2.0379, -2.0969, -2.1204, -2.1318],\n",
       "                      [-0.7024, -2.0996, -2.0831, -2.0956, -2.0196],\n",
       "                      [-0.7606, -2.1473, -2.1223, -2.1562, -2.1031],\n",
       "                      [-0.6643, -2.0510, -2.0510, -2.0510, -2.1340],\n",
       "                      [-0.8341, -2.2191, -2.2191, -2.2191, -2.1100],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6982, -2.0845, -2.0845, -2.0845, -1.8638],\n",
       "                      [-0.4386, -1.8255, -1.8255, -1.8255, -2.0653],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.8279, -2.2142, -2.2142, -2.2102, -2.2129],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.5800, -1.9663, -1.9663, -1.9663, -2.0472],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6507, -2.0373, -2.0373, -2.0373, -2.1033],\n",
       "                      [-0.6931, -2.0794, -2.0794, -2.0794, -2.0794],\n",
       "                      [-0.6952, -2.0815, -2.0815, -2.0815, -2.0973],\n",
       "                      [-0.6925, -2.0788, -2.0788, -2.0788, -2.0323],\n",
       "                      [-0.7421, -2.1284, -2.1284, -2.1284, -2.0404],\n",
       "                      [-0.6420, -2.0283, -2.0283, -2.0283, -1.9408],\n",
       "                      [-0.8100, -2.1962, -2.1962, -2.1962, -2.1780],\n",
       "                      [-0.7238, -2.1101, -2.1101, -2.1101, -2.0830]])),\n",
       "             ('pipeline.0.cat_tf_prob_logits',\n",
       "              tensor([[-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.7588, -0.5940],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-1.2047, -0.1816],\n",
       "                      [-0.6931, -0.6931],\n",
       "                      [-0.6931, -0.6931]])),\n",
       "             ('pipeline.1.tf_prob_logits',\n",
       "              tensor([[-0.2380, -2.2218, -2.2226, -2.3356],\n",
       "                      [-1.8715, -0.5778, -0.6779, -2.9316],\n",
       "                      [-1.8909, -1.2870, -1.2865, -2.0325],\n",
       "                      [-1.3824, -1.4400, -1.3156, -1.8846],\n",
       "                      [-0.8799, -1.6627, -1.6614, -1.8918],\n",
       "                      [-1.3494, -1.2085, -1.1110, -2.4193],\n",
       "                      [-1.1981, -0.9528, -2.4303, -2.0263],\n",
       "                      [-0.1704, -1.7542, -1.7567, -2.6279],\n",
       "                      [-0.9111, -1.5915, -1.5917, -1.8726],\n",
       "                      [-0.7711, -1.6763, -1.6746, -2.3078],\n",
       "                      [-0.7239, -1.7620, -1.7620, -1.7990],\n",
       "                      [-0.7022, -1.8084, -1.7943, -1.7582],\n",
       "                      [-0.1165, -2.4558, -2.4558, -1.0724],\n",
       "                      [-0.7662, -1.8822, -1.8822, -1.7183],\n",
       "                      [-0.4367, -2.0076, -2.0081, -2.1079],\n",
       "                      [-1.4194, -1.0637, -1.0629, -2.6036],\n",
       "                      [-1.2805, -1.1835, -1.1833, -2.5562],\n",
       "                      [-2.3088, -0.3730, -3.3359, -0.2117],\n",
       "                      [-0.8263, -1.6476, -1.6483, -1.9417],\n",
       "                      [-0.5535, -2.2035, -0.5690, -2.3506],\n",
       "                      [-0.6532, -1.8307, -1.8310, -1.7543],\n",
       "                      [-0.6501, -1.8356, -1.7997, -1.7545],\n",
       "                      [-0.7705, -1.7470, -1.9133, -1.7577],\n",
       "                      [-0.6950, -1.6748, -1.9240, -1.6459],\n",
       "                      [-0.4047, -2.1750, -1.3795, -1.7739],\n",
       "                      [-1.6529, -1.0362, -1.0305, -2.3437],\n",
       "                      [-0.4607, -1.6601, -1.6624, -1.9812],\n",
       "                      [-0.8463, -1.6484, -1.6486, -1.8541],\n",
       "                      [-0.7009, -1.9088, -1.9088, -1.6531],\n",
       "                      [-0.4737, -1.5607, -1.5607, -2.0160],\n",
       "                      [-1.5735, -0.6006, -1.0705, -3.2566],\n",
       "                      [-1.3654, -1.1279, -1.0943, -2.5133],\n",
       "                      [-0.8488, -1.6383, -1.6243, -1.9408],\n",
       "                      [-0.5190, -1.9676, -1.9508, -1.6671],\n",
       "                      [-1.5651, -2.1673, -0.6300, -2.2807],\n",
       "                      [-0.2428, -2.0504, -2.2853, -1.8409],\n",
       "                      [-0.7675, -1.7011, -1.7287, -1.8671],\n",
       "                      [-1.3137, -1.1712, -1.1712, -1.1712],\n",
       "                      [-1.0661, -1.4188, -1.4188, -1.4188],\n",
       "                      [-0.9852, -1.4998, -1.4998, -1.4998],\n",
       "                      [-0.6568, -1.8281, -1.8281, -1.8281],\n",
       "                      [-1.0046, -1.4803, -1.4803, -1.4803],\n",
       "                      [-0.9434, -1.5415, -1.5415, -1.5415],\n",
       "                      [-1.1513, -1.3336, -1.3336, -1.3336],\n",
       "                      [-1.3547, -1.1302, -1.1302, -1.1302],\n",
       "                      [-0.5012, -1.9837, -1.9837, -1.9837],\n",
       "                      [-0.1934, -2.2915, -2.2915, -2.2915],\n",
       "                      [-1.3535, -1.1314, -1.1314, -1.1314],\n",
       "                      [-1.1888, -1.2961, -1.2961, -1.2961],\n",
       "                      [-1.1401, -1.3448, -1.3448, -1.3448],\n",
       "                      [-0.8171, -1.6678, -1.6678, -1.6678],\n",
       "                      [ 0.0593, -2.5442, -2.5442, -2.5442],\n",
       "                      [-0.6291, -1.8558, -1.8558, -1.8558],\n",
       "                      [-0.1527, -2.3322, -2.3322, -2.3322],\n",
       "                      [-0.2309, -2.2540, -2.2540, -2.2540],\n",
       "                      [-1.3851, -1.0998, -1.0998, -1.0998],\n",
       "                      [-1.5098, -0.9751, -0.9751, -0.9751],\n",
       "                      [-0.6817, -1.8033, -1.8033, -1.8033],\n",
       "                      [-0.6601, -1.8248, -1.8248, -1.8248],\n",
       "                      [-0.1304, -2.3545, -2.3545, -2.3545],\n",
       "                      [-0.0902, -2.3947, -2.3947, -2.3947],\n",
       "                      [-0.3084, -2.1765, -2.1765, -2.1765],\n",
       "                      [-0.2735, -2.2114, -2.2114, -2.2114],\n",
       "                      [-0.9296, -1.5553, -1.5553, -1.5553],\n",
       "                      [-0.5152, -1.9697, -1.9697, -1.9697],\n",
       "                      [-0.9363, -1.5486, -1.5486, -1.5486],\n",
       "                      [-0.1864, -2.2985, -2.2985, -2.2985],\n",
       "                      [ 0.0907, -2.5756, -2.5756, -2.5756],\n",
       "                      [-0.7502, -1.7347, -1.7347, -1.7347],\n",
       "                      [-0.3843, -2.1006, -2.1006, -2.1006],\n",
       "                      [ 0.1306, -2.6155, -2.6155, -2.6155],\n",
       "                      [-0.0396, -2.4453, -2.4453, -2.4453],\n",
       "                      [-1.3432, -1.1417, -1.1417, -1.1417],\n",
       "                      [-0.0211, -2.4638, -2.4638, -2.4638],\n",
       "                      [-0.5912, -1.8937, -1.8937, -1.8937],\n",
       "                      [-0.6969, -1.7880, -1.7880, -1.7880],\n",
       "                      [-1.3022, -1.1827, -1.1827, -1.1827],\n",
       "                      [-0.0566, -2.4283, -2.4283, -2.4283],\n",
       "                      [-0.6157, -1.8692, -1.8692, -1.8692],\n",
       "                      [-1.3185, -1.1664, -1.1664, -1.1664],\n",
       "                      [-0.9058, -1.5791, -1.5791, -1.5791],\n",
       "                      [-1.2470, -1.2379, -1.2379, -1.2379],\n",
       "                      [-0.0962, -2.3887, -2.3887, -2.3887],\n",
       "                      [ 0.1778, -2.6627, -2.6627, -2.6627],\n",
       "                      [-0.6188, -1.8661, -1.8661, -1.8661],\n",
       "                      [-0.4705, -2.0144, -2.0144, -2.0144],\n",
       "                      [-0.5036, -1.9813, -1.9813, -1.9813],\n",
       "                      [-1.4329, -1.0520, -1.0520, -1.0520],\n",
       "                      [-0.9844, -1.5005, -1.5005, -1.5005],\n",
       "                      [-0.7666, -1.7183, -1.7183, -1.7183],\n",
       "                      [-0.4898, -1.9951, -1.9951, -1.9951],\n",
       "                      [ 0.1957, -2.6806, -2.6806, -2.6806],\n",
       "                      [-0.3258, -2.1591, -2.1591, -2.1591],\n",
       "                      [-0.3585, -2.1264, -2.1264, -2.1264]])),\n",
       "             ('pipeline.2.tf_prob_logits',\n",
       "              tensor([[-3.6223, -3.6859, -3.9413, -2.1141, -2.1141, -2.1141, -2.1141, -2.1141,\n",
       "                       -2.1141, -1.4251],\n",
       "                      [-1.9503, -1.9656, -1.9971, -3.7981, -3.7981, -3.7981, -1.9597, -1.9677,\n",
       "                       -2.4840,  0.2470],\n",
       "                      [-2.1095, -2.1095, -2.6512, -3.5159, -3.5159, -3.5159, -3.2661, -3.8608,\n",
       "                       -3.1970,  0.0878],\n",
       "                      [-3.3960, -3.3887, -3.4148, -2.2647, -2.5162, -3.2891, -3.1653, -1.8428,\n",
       "                       -2.1225, -1.1988],\n",
       "                      [-3.1224, -3.1230, -2.9670, -3.0887, -2.6890, -2.3724, -2.9029, -3.0069,\n",
       "                       -2.9952, -0.9251],\n",
       "                      [-3.1833, -3.1833, -3.5192, -3.5647, -2.2525, -2.2525, -3.1833, -3.1833,\n",
       "                       -3.1833, -0.9861],\n",
       "                      [-2.7360, -2.7360, -2.7360, -2.9835, -2.9835, -2.9835, -2.7360, -2.7360,\n",
       "                       -2.7360, -0.5386],\n",
       "                      [-2.6861, -2.6861, -2.6861, -2.3678, -2.3144, -3.2882, -2.6861, -2.6861,\n",
       "                       -2.6861, -0.4889],\n",
       "                      [-2.9288, -2.9285, -3.5942, -3.4344, -3.0403, -2.4601, -2.9287, -2.9497,\n",
       "                       -3.4214, -0.7315],\n",
       "                      [-2.7448, -2.7213, -2.1874, -2.3657, -3.4603, -3.4720, -2.7213, -2.9221,\n",
       "                       -2.2088, -0.5476],\n",
       "                      [-2.9371, -2.9375, -2.5470, -2.5539, -2.9373, -2.9470, -2.9371, -2.9375,\n",
       "                       -2.5538, -0.7399],\n",
       "                      [-3.3636, -3.3636, -3.3636, -3.3636, -3.0929, -2.4370, -3.3636, -3.3636,\n",
       "                       -3.3636, -1.1664],\n",
       "                      [-3.5303, -3.5304, -3.1895, -1.9975, -2.2082, -2.9090, -3.5012, -3.1972,\n",
       "                       -2.0526, -1.3330],\n",
       "                      [-2.7773, -2.7773, -2.7998, -2.9459, -2.9459, -2.9459, -2.9690, -2.9714,\n",
       "                       -2.9983, -0.5657],\n",
       "                      [-2.9080, -2.9083, -2.1998, -2.8345, -3.4005, -3.2318, -2.2006, -2.2238,\n",
       "                       -2.3189, -0.7109],\n",
       "                      [-3.3171, -3.3167, -3.2961, -3.2949, -3.3187, -2.1791, -3.3169, -3.4669,\n",
       "                       -3.2931, -1.1199],\n",
       "                      [-2.2587, -2.2586, -3.7197, -3.7189, -3.7330, -2.6479, -2.2586, -2.4248,\n",
       "                       -3.7185, -0.0614],\n",
       "                      [-4.2124, -4.2124, -4.2132, -3.7991, -1.2607, -1.6018, -4.0393, -3.8973,\n",
       "                       -4.0332, -2.0152],\n",
       "                      [-3.0943, -3.0943, -3.1040, -3.7715, -2.9289, -2.2148, -3.8700, -3.7673,\n",
       "                       -2.6330, -0.8971],\n",
       "                      [-3.2077, -3.1990, -3.1979, -2.7840, -2.6627, -2.5280, -3.2181, -3.2991,\n",
       "                       -2.7838, -1.0105],\n",
       "                      [-2.9665, -2.9664, -2.9767, -2.7620, -2.9686, -2.5437, -3.1953, -2.4846,\n",
       "                       -3.2710, -0.7693],\n",
       "                      [-2.5365, -2.5365, -2.5365, -3.2046, -3.1943, -3.2500, -2.5365, -2.5365,\n",
       "                       -2.7252, -0.3393],\n",
       "                      [-2.1039, -2.1039, -2.1110, -2.9235, -3.5194, -3.7043, -2.1257, -2.2820,\n",
       "                       -3.8680,  0.0934],\n",
       "                      [-3.1556, -3.1556, -3.1439, -3.0070, -2.5306, -2.4839, -3.1556, -3.1558,\n",
       "                       -2.5292, -0.9584],\n",
       "                      [-2.2483, -2.2482, -2.2966, -2.2558, -2.2268, -3.4878, -2.2583, -2.2932,\n",
       "                       -2.2451, -0.0814],\n",
       "                      [-3.5034, -3.5034, -3.5035, -2.2730, -2.2764, -2.2907, -2.3017, -2.2639,\n",
       "                       -2.2604, -1.3062],\n",
       "                      [-3.2053, -3.2053, -3.2053, -3.2431, -2.4452, -2.7142, -3.2053, -3.2053,\n",
       "                       -3.2826, -1.0081],\n",
       "                      [-2.9333, -2.9331, -3.4193, -3.3434, -2.9117, -2.6619, -2.9333, -2.9716,\n",
       "                       -3.3146, -0.7361],\n",
       "                      [-2.8007, -2.8219, -2.9171, -3.0272, -3.0147, -3.0113, -2.9171, -2.9588,\n",
       "                       -2.9643, -0.6035],\n",
       "                      [-3.1414, -3.1371, -3.1371, -2.6608, -2.6470, -2.6456, -2.6192, -2.6195,\n",
       "                       -2.6462, -0.9442],\n",
       "                      [-1.8962, -1.8962, -1.9003, -3.2994, -3.3290, -3.7897, -1.8962, -1.8917,\n",
       "                       -3.1077,  0.2997],\n",
       "                      [-3.5993, -3.6088, -2.5530, -2.1919, -2.5453, -2.3949, -3.5638, -2.7796,\n",
       "                       -1.7891, -1.4021],\n",
       "                      [-3.1450, -3.1448, -3.1205, -3.0038, -2.7362, -2.6099, -3.1448, -3.1448,\n",
       "                       -3.0331, -0.9476],\n",
       "                      [-2.8204, -2.8204, -2.7672, -2.7672, -2.9828, -2.9123, -2.8204, -2.8204,\n",
       "                       -2.7678, -0.6239],\n",
       "                      [-2.0418, -2.0419, -3.5834, -3.4356, -3.6444, -3.2543, -2.0418, -3.6494,\n",
       "                       -3.4973,  0.1555],\n",
       "                      [-3.8621, -3.8621, -3.8782, -1.9687, -1.8507, -2.0772, -3.8621, -3.8738,\n",
       "                       -3.9109, -1.6648],\n",
       "                      [-2.9428, -2.9428, -2.9441, -2.9102, -2.9063, -2.7587, -2.9428, -2.9428,\n",
       "                       -2.9440, -0.7456],\n",
       "                      [-3.4945, -3.4945, -3.4945, -3.4945, -3.4945, -3.4945, -3.4945, -3.4945,\n",
       "                       -3.4945, -0.0779],\n",
       "                      [-2.8797, -2.8797, -2.8797, -2.8797, -2.8797, -2.8797, -2.8797, -2.8797,\n",
       "                       -2.8797, -0.7278],\n",
       "                      [-2.8176, -2.8176, -2.8176, -2.8176, -2.8176, -2.8176, -2.8176, -2.8176,\n",
       "                       -2.8176, -0.6759],\n",
       "                      [-1.6863, -1.6863, -1.6863, -1.6863, -1.6863, -1.6863, -1.6863, -1.6863,\n",
       "                       -1.6863, -1.9860],\n",
       "                      [-2.9114, -2.9114, -2.9114, -2.9114, -2.9114, -2.9114, -2.9114, -2.9114,\n",
       "                       -2.9114, -0.8108],\n",
       "                      [-3.1873, -3.1873, -3.1873, -3.1873, -3.1873, -3.1873, -3.1873, -3.1873,\n",
       "                       -3.1873, -0.4007],\n",
       "                      [-2.9057, -2.9057, -2.9057, -2.9057, -2.9057, -2.9057, -2.9057, -2.9057,\n",
       "                       -2.9057, -0.7500],\n",
       "                      [-2.8233, -2.8233, -2.8233, -2.8233, -2.8233, -2.8233, -2.8233, -2.8233,\n",
       "                       -2.8233, -0.5822],\n",
       "                      [-3.0891, -3.0891, -3.0891, -3.0891, -3.0891, -3.0891, -3.0891, -3.0891,\n",
       "                       -3.0891, -0.4935],\n",
       "                      [-3.0329, -3.0329, -3.0329, -3.0329, -3.0329, -3.0329, -3.0329, -3.0329,\n",
       "                       -3.0329, -0.7876],\n",
       "                      [-3.6037, -3.6037, -3.6037, -3.6037, -3.6037, -3.6037, -3.6037, -3.6037,\n",
       "                       -3.6037, -0.0197],\n",
       "                      [-3.0794, -1.8403, -1.8403, -1.8403, -1.8403, -1.8403, -1.8403, -1.8403,\n",
       "                       -1.8403, -1.7040],\n",
       "                      [-2.7552, -2.7552, -2.7552, -2.7552, -2.7552, -2.7552, -2.7552, -2.7552,\n",
       "                       -2.7552, -0.9377],\n",
       "                      [-2.8994, -2.8994, -2.8994, -2.8994, -2.8994, -2.8994, -2.8994, -2.8994,\n",
       "                       -2.8994, -0.7340],\n",
       "                      [-2.7734, -2.7734, -2.7734, -2.7734, -2.7734, -2.7734, -2.7734, -2.7734,\n",
       "                       -2.7734, -0.5735],\n",
       "                      [-2.9228, -2.9228, -2.9228, -2.9228, -2.9228, -2.9228, -2.9228, -2.9228,\n",
       "                       -2.9228, -0.7337],\n",
       "                      [-2.3656, -2.3656, -2.3656, -2.3656, -2.3656, -2.3656, -2.3656, -2.3656,\n",
       "                       -2.3656, -1.3465],\n",
       "                      [-2.9559, -2.9559, -2.9559, -2.9559, -2.9559, -2.9559, -2.9559, -2.9559,\n",
       "                       -2.9559, -0.7184],\n",
       "                      [-2.6643, -2.6643, -2.6643, -2.6643, -2.6643, -2.6643, -2.6643, -2.6643,\n",
       "                       -2.6643, -0.9547],\n",
       "                      [-3.1299, -3.1299, -3.1299, -3.1299, -3.1299, -3.1299, -3.1299, -3.1299,\n",
       "                       -3.1299, -0.4411],\n",
       "                      [-2.4610, -2.4610, -2.4610, -2.4610, -2.4610, -2.4610, -2.4610, -2.4610,\n",
       "                       -2.4610, -0.9591],\n",
       "                      [-2.7290, -2.7290, -2.7290, -2.7290, -2.7290, -2.7290, -2.7290, -2.7290,\n",
       "                       -2.7290, -0.7586],\n",
       "                      [-2.8981, -2.8981, -2.8981, -2.8981, -2.8981, -2.8981, -2.8981, -2.8981,\n",
       "                       -2.8981, -0.6731],\n",
       "                      [-3.2756, -3.2756, -3.2756, -3.2756, -3.2756, -3.2756, -3.2756, -3.2756,\n",
       "                       -3.2756, -0.9009],\n",
       "                      [-2.9342, -2.9342, -2.9342, -2.9342, -2.9342, -2.9342, -2.9342, -2.9342,\n",
       "                       -2.9342, -0.6734],\n",
       "                      [-2.8776, -2.8776, -2.8776, -2.8776, -2.8776, -2.8776, -2.8776, -2.8776,\n",
       "                       -2.8776, -0.6000],\n",
       "                      [-2.0652, -2.0652, -2.0652, -2.0652, -2.0652, -2.0652, -2.0652, -2.0652,\n",
       "                       -2.0652, -1.3397],\n",
       "                      [-2.7695, -2.7695, -2.7695, -2.7695, -2.7695, -2.7695, -2.7695, -2.7695,\n",
       "                       -2.7695, -0.7520],\n",
       "                      [-2.3431, -2.3431, -2.3431, -2.3431, -2.3431, -2.3431, -2.3431, -2.3431,\n",
       "                       -2.3431, -1.2242],\n",
       "                      [-2.9091, -2.9091, -2.9091, -2.9091, -2.9091, -2.9091, -2.9091, -2.9091,\n",
       "                       -2.9091, -0.6622],\n",
       "                      [-3.1179, -3.1179, -3.1179, -3.1179, -3.1179, -3.1179, -3.1179, -3.1179,\n",
       "                       -3.1179, -0.7641],\n",
       "                      [-2.6590, -2.6590, -2.6590, -2.6590, -2.6590, -2.6590, -2.6590, -2.6590,\n",
       "                       -2.6590, -0.8127],\n",
       "                      [-2.5483, -3.1266, -3.2383, -3.2383, -3.2383, -3.2383, -3.2383, -3.2383,\n",
       "                       -3.2383, -0.3511],\n",
       "                      [-2.1795, -3.9494, -3.9494, -3.9494, -3.9494, -3.9494, -3.9494, -3.9494,\n",
       "                       -3.9494,  0.3594],\n",
       "                      [-2.8985, -2.8985, -2.8985, -2.8985, -2.8985, -2.8985, -2.8985, -2.8985,\n",
       "                       -2.8985, -0.6621],\n",
       "                      [-2.2849, -2.2223, -2.2223, -2.2223, -2.2223, -2.2223, -2.2223, -2.2223,\n",
       "                       -2.2223, -1.3564],\n",
       "                      [-3.6267, -3.6267, -3.6267, -3.6267, -3.6267, -3.6267, -3.6267, -3.6267,\n",
       "                       -3.6267,  0.0432],\n",
       "                      [-2.5564, -2.5564, -2.5564, -2.5564, -2.5564, -2.5564, -2.5564, -2.5564,\n",
       "                       -2.5564, -1.0693],\n",
       "                      [-2.8706, -2.8706, -2.8706, -2.8706, -2.8706, -2.8706, -2.8706, -2.8706,\n",
       "                       -2.8706, -0.7218],\n",
       "                      [-2.7700, -2.7700, -2.7700, -2.7700, -2.7700, -2.7700, -2.7700, -2.7700,\n",
       "                       -2.7700, -0.8690],\n",
       "                      [-2.8293, -2.8293, -2.8293, -2.8293, -2.8293, -2.8293, -2.8293, -2.8293,\n",
       "                       -2.8293, -0.5630],\n",
       "                      [-2.9228, -2.9228, -2.9228, -2.9228, -2.9228, -2.9228, -2.9228, -2.9228,\n",
       "                       -2.9228, -0.6825],\n",
       "                      [-1.4870, -1.4870, -1.4870, -1.4870, -1.4870, -1.4870, -1.4870, -1.4870,\n",
       "                       -1.4870, -2.1585],\n",
       "                      [-3.2800, -3.2800, -2.5048, -2.5001, -2.5001, -2.5001, -2.5001, -2.5001,\n",
       "                       -2.5001, -1.0828],\n",
       "                      [-3.7052, -3.7025, -2.0757, -2.0757, -2.0757, -2.0757, -2.0757, -2.0757,\n",
       "                       -2.0757, -1.5080],\n",
       "                      [-3.0034, -3.0034, -3.0034, -3.0034, -3.0034, -3.0034, -3.0034, -3.0034,\n",
       "                       -3.0034, -0.5934],\n",
       "                      [-2.0814, -4.4239, -4.4239, -4.4239, -4.4239, -4.4239, -4.4239, -4.4239,\n",
       "                       -4.4239,  0.8627],\n",
       "                      [-2.9250, -2.9250, -2.9250, -2.9250, -2.9250, -2.9250, -2.9250, -2.9250,\n",
       "                       -2.9250, -0.7230],\n",
       "                      [-3.2426, -3.2426, -3.2426, -3.2426, -3.2426, -3.2426, -3.2426, -3.2426,\n",
       "                       -3.2426, -0.4439],\n",
       "                      [-2.9684, -2.9684, -2.9684, -2.9684, -2.9684, -2.9684, -2.9684, -2.9684,\n",
       "                       -2.9684, -0.5760],\n",
       "                      [-2.9702, -2.9702, -2.9702, -2.9702, -2.9702, -2.9702, -2.9702, -2.9702,\n",
       "                       -2.9702, -0.6866],\n",
       "                      [-2.8488, -2.8488, -2.8488, -2.8488, -2.8488, -2.8488, -2.8488, -2.8488,\n",
       "                       -2.8488, -0.7150],\n",
       "                      [-3.1215, -2.6570, -2.6570, -2.6570, -2.6570, -2.6570, -2.6570, -2.6570,\n",
       "                       -2.6570, -0.9267],\n",
       "                      [-2.8542, -2.8542, -2.8542, -2.8542, -2.8542, -2.8542, -2.8542, -2.8542,\n",
       "                       -2.8542, -0.6320],\n",
       "                      [-3.3747, -3.3747, -3.3747, -3.3747, -3.3747, -3.3747, -3.3747, -3.3747,\n",
       "                       -3.3747, -0.6154],\n",
       "                      [-2.8728, -2.8728, -2.8728, -2.8728, -2.8728, -2.8728, -2.8728, -2.8728,\n",
       "                       -2.9418, -0.6770],\n",
       "                      [-2.9088, -2.9088, -2.9088, -2.9088, -2.9088, -2.9088, -2.9088, -2.9088,\n",
       "                       -2.9088, -0.7056]])),\n",
       "             ('pipeline.3.tf_prob_logits',\n",
       "              tensor([[-2.2526, -2.9472, -2.9327, -2.7349, -2.3768, -2.3178, -0.6309],\n",
       "                      [-3.8685, -4.2066, -1.9514, -4.1685, -2.0679, -0.7359, -2.0376],\n",
       "                      [-2.7190, -3.4411, -2.8655, -1.4101, -1.9804, -2.1535, -0.9445],\n",
       "                      [-3.6033, -3.4763, -1.4120, -3.8647, -2.1317, -1.2334, -1.7139],\n",
       "                      [-2.3550, -4.0238, -3.6897, -4.0175, -3.5997, -3.5371,  0.5483],\n",
       "                      [-1.9903, -2.4380, -2.7433, -3.1798, -3.1908, -3.0400, -0.2620],\n",
       "                      [-2.4463, -3.2009, -3.2842, -3.0866, -3.5125, -3.5961,  0.2965],\n",
       "                      [-3.3277, -3.1151, -2.9315, -2.5751, -2.4007, -2.5202, -0.3309],\n",
       "                      [-3.0779, -3.6962, -3.5361, -2.5232, -3.5954, -3.5079,  0.4625],\n",
       "                      [-4.0200, -3.2253, -1.0136, -4.0625, -3.1158, -0.8667, -2.0497],\n",
       "                      [-2.2361, -3.0631, -2.9978, -2.6609, -3.0173, -3.0035, -0.1278],\n",
       "                      [-1.7873, -2.0319, -2.5407, -3.2483, -3.0688, -3.0089, -0.1353],\n",
       "                      [-1.5645, -3.9609, -3.9619, -3.4270, -3.7160, -3.6865,  0.6675],\n",
       "                      [-2.4828, -2.5804, -2.4077, -2.3588, -2.4329, -2.3932, -0.7360],\n",
       "                      [-2.4643, -3.5414, -3.4221, -1.8513, -3.1164, -3.0500,  0.1334],\n",
       "                      [-3.7532, -4.0233, -3.9078, -3.0077, -3.9508, -3.8546,  0.8142],\n",
       "                      [-3.0523, -4.0077, -3.8417, -3.9152, -3.8934, -3.8376,  0.7791],\n",
       "                      [-1.1797, -2.7363, -2.9993, -3.5275, -3.9470, -3.7719,  0.2013],\n",
       "                      [-0.6804, -1.5101, -1.9860, -3.6104, -3.9380, -3.8041, -0.6621],\n",
       "                      [-2.7527, -2.8646, -2.8098, -1.9075, -2.3660, -2.7340, -0.3849],\n",
       "                      [-1.3150, -3.6805, -3.4165, -2.6137, -3.5302, -3.4884,  0.3137],\n",
       "                      [-2.3527, -2.4451, -2.5910, -2.6600, -2.8661, -2.8146, -0.4817],\n",
       "                      [-0.8416, -2.0496, -1.9782, -3.2987, -3.5379, -3.3803, -0.7878],\n",
       "                      [-2.6490, -3.1155, -3.1991, -3.2057, -3.7962, -3.7760,  0.4009],\n",
       "                      [-0.5371, -2.0413, -2.1153, -3.0366, -3.4147, -3.0905, -0.9732],\n",
       "                      [-1.7042, -2.9135, -3.6451, -3.6515, -3.2167, -3.1647,  0.1655],\n",
       "                      [-2.4209, -2.4374, -2.5198, -2.1955, -2.5636, -2.6559, -0.5314],\n",
       "                      [-3.4106, -3.8610, -3.7449, -2.8650, -3.8474, -3.7469,  0.7057],\n",
       "                      [-2.4390, -2.3411, -2.5393, -2.7378, -2.7703, -2.7521, -0.4705],\n",
       "                      [-2.4679, -2.4678, -2.4640, -1.6945, -2.4875, -2.4290, -0.8863],\n",
       "                      [-2.3197, -3.4221, -3.3878, -1.9403, -3.2607, -3.3684,  0.2032],\n",
       "                      [-2.6213, -2.7037, -2.5515, -2.0805, -2.0753, -2.2302, -0.8416],\n",
       "                      [-2.2469, -2.6320, -2.7124, -2.5433, -2.8306, -2.8315, -0.3844],\n",
       "                      [-2.2529, -2.5757, -2.6625, -2.1593, -2.6806, -2.6944, -0.5043],\n",
       "                      [-3.7537, -3.7616, -2.1518, -3.0457, -1.5865, -1.1348, -1.6557],\n",
       "                      [-2.7846, -3.6536, -3.4361, -1.6883, -3.3931, -3.3492,  0.2744],\n",
       "                      [-2.3109, -2.6617, -2.6765, -2.3617, -2.6843, -2.6720, -0.4975],\n",
       "                      [-4.0054, -3.9436, -0.8168, -4.0054, -3.9436, -0.8168, -2.2667],\n",
       "                      [-1.9502, -2.5756, -3.0021, -1.9502, -2.5756, -3.0021, -0.5308],\n",
       "                      [-1.9180, -2.9540, -3.1519, -1.9180, -2.9540, -3.1519, -0.1866],\n",
       "                      [-3.3011, -3.9732, -3.7957, -3.3011, -3.9732, -3.7957,  0.8013],\n",
       "                      [-1.9631, -3.2210, -3.0538, -1.9631, -3.2210, -3.0538, -0.0835],\n",
       "                      [-3.9763, -3.3011, -0.9313, -3.9763, -3.3011, -0.9313, -2.0621],\n",
       "                      [-1.8129, -2.8403, -3.2728, -1.8129, -2.8403, -3.2728, -0.2607],\n",
       "                      [-2.8453, -2.0606, -2.0859, -2.8453, -2.0606, -2.0859, -1.0991],\n",
       "                      [-3.9477, -3.1583, -0.9614, -3.9477, -3.1583, -0.9614, -1.9822],\n",
       "                      [-2.7600, -2.7036, -2.3062, -2.7600, -2.7036, -2.3062, -0.7302],\n",
       "                      [-3.8409, -3.7008, -0.9626, -3.8409, -3.7008, -0.9626, -2.1000],\n",
       "                      [-3.7832, -4.1147, -3.9598, -3.7832, -4.1147, -3.9658,  0.9281],\n",
       "                      [-2.0050, -3.4743, -3.2612, -2.0050, -3.4743, -3.2612,  0.2291],\n",
       "                      [-2.3625, -2.4224, -2.5687, -2.3625, -2.4224, -2.5687, -0.6798],\n",
       "                      [-3.1718, -3.2449, -2.2957, -3.1718, -3.2449, -2.2957, -0.2114],\n",
       "                      [-2.4036, -2.5463, -2.6643, -2.4036, -2.5463, -2.6643, -0.5808],\n",
       "                      [-2.3865, -3.8263, -3.7015, -2.3865, -3.8263, -3.7015,  0.6418],\n",
       "                      [-2.9930, -2.9494, -2.5025, -2.9930, -2.9494, -2.5025, -0.3181],\n",
       "                      [-2.0434, -3.2956, -3.2295, -2.0434, -3.2956, -3.2295,  0.1228],\n",
       "                      [-3.6995, -3.3202, -1.0703, -3.6995, -3.3202, -1.0703, -1.9450],\n",
       "                      [-2.0059, -3.5628, -3.0940, -2.0059, -3.5628, -3.0940,  0.0317],\n",
       "                      [-2.2947, -2.5479, -2.6261, -2.2947, -2.5479, -2.6261, -0.5896],\n",
       "                      [-3.0006, -3.1374, -2.2001, -3.0006, -3.1374, -2.2001, -0.4351],\n",
       "                      [-3.1897, -3.1127, -1.8603, -3.1897, -3.1127, -1.8603, -0.6811],\n",
       "                      [-2.9084, -2.9127, -2.4592, -2.9084, -2.9127, -2.4592, -0.3463],\n",
       "                      [-3.0589, -2.8962, -2.0949, -3.0589, -2.8962, -2.0949, -0.5002],\n",
       "                      [-2.0683, -3.7144, -3.4255, -2.0683, -3.7144, -3.4255,  0.4593],\n",
       "                      [-2.3637, -2.5462, -2.5773, -2.3637, -2.5462, -2.5773, -0.6206],\n",
       "                      [-2.3325, -3.1667, -3.0525, -2.3325, -3.1667, -3.0448, -0.0577],\n",
       "                      [-3.2371, -3.0562, -1.6207, -3.2371, -3.0562, -1.6207, -1.4265],\n",
       "                      [-3.2377, -3.2104, -1.9493, -3.2377, -3.2104, -1.9493, -0.4849],\n",
       "                      [-2.0608, -2.9118, -2.8630, -2.0608, -2.9118, -2.8630, -0.3163],\n",
       "                      [-2.8713, -2.2407, -2.1571, -2.8713, -2.2470, -2.1605, -0.9919],\n",
       "                      [-3.5534, -2.6497, -1.4191, -3.5534, -2.6497, -1.4427, -1.6079],\n",
       "                      [-3.0373, -3.0470, -2.2154, -3.0373, -3.0470, -2.2154, -0.4504],\n",
       "                      [-2.8528, -3.5295, -3.4168, -2.8528, -3.5295, -3.4209,  0.3463],\n",
       "                      [-3.5326, -2.7147, -1.4365, -3.5326, -2.7147, -1.4418, -1.5956],\n",
       "                      [-2.1078, -3.2059, -2.9366, -2.1078, -3.2059, -2.9366, -0.1636],\n",
       "                      [-2.2876, -2.8208, -2.7585, -2.2876, -2.8208, -2.7585, -0.3934],\n",
       "                      [-2.1552, -3.5582, -3.3829, -2.1552, -3.5582, -3.3829,  0.3595],\n",
       "                      [-3.2936, -3.0321, -1.6851, -3.2936, -3.0321, -1.6851, -0.8135],\n",
       "                      [-2.5996, -2.3438, -2.3423, -2.5996, -2.3438, -2.3423, -0.8453],\n",
       "                      [-3.3387, -4.1535, -3.9957, -3.3387, -4.1535, -3.9957,  1.0056],\n",
       "                      [-2.3151, -2.9170, -2.8590, -2.3166, -2.7877, -2.7869, -0.3300],\n",
       "                      [-3.3352, -3.7672, -3.5979, -3.3352, -3.8293, -3.6109,  0.5963],\n",
       "                      [-3.1898, -2.9119, -1.7712, -3.1898, -2.9119, -1.7712, -1.3095],\n",
       "                      [-4.0778, -3.3632, -0.8104, -4.0778, -3.3632, -0.9128, -2.0944],\n",
       "                      [-2.4241, -2.5559, -2.6029, -2.4241, -2.5559, -2.6029, -0.5687],\n",
       "                      [-2.9169, -2.5187, -2.0958, -2.9169, -2.5187, -2.0958, -0.9871],\n",
       "                      [-2.6975, -2.3361, -2.2275, -2.6975, -2.3361, -2.2275, -0.9573],\n",
       "                      [-3.2358, -2.2973, -1.6028, -3.2358, -2.2973, -1.6028, -1.4955],\n",
       "                      [-1.9920, -3.2745, -3.2330, -1.9920, -3.2745, -3.2330,  0.1490],\n",
       "                      [-2.3064, -2.7558, -2.7262, -2.3064, -2.7558, -2.7270, -0.4363],\n",
       "                      [-2.6911, -2.6372, -2.3655, -2.6911, -2.6372, -2.3655, -0.6717],\n",
       "                      [-3.1938, -2.5291, -1.8536, -3.1938, -2.5291, -1.8536, -1.1899],\n",
       "                      [-2.9821, -2.8230, -2.1544, -2.9821, -2.8230, -2.1544, -0.5795],\n",
       "                      [-2.8641, -2.7703, -2.3303, -2.8641, -2.7703, -2.3303, -0.5454]]))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model['prep_pipeline']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffprep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
