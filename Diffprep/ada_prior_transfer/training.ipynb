{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddharth/miniconda3/envs/diffprep/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "from experiment.experiment_utils import set_random_seed, load_data, build_data, grid_search, makedir, save_result, load_data_multitask, load_data_multitask_synthetic_label, save_task_label\n",
    "from model import LogisticRegression\n",
    "from pipeline.diffprep_flex_pipeline import DiffPrepFlexPipeline\n",
    "from pipeline.diffprep_fix_pipeline import DiffPrepFixPipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from trainer.diffprep_trainer import DiffPrepSGD\n",
    "from utils import SummaryWriter\n",
    "from experiment.experiment_utils import min_max_normalize\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# old_df = pd.read_csv('./data/ada_prior/data_old.csv')\n",
    "# #sns.distplot(old_df['hoursPerWeek'])\n",
    "# #old_df['hoursPerWeek'] = (old_df['hoursPerWeek'] <= 40).map({True: 'Y', False: 'N'})\n",
    "# old_df['label'] = old_df['label'].map({-1: 'N', 1: 'Y'})\n",
    "# old_df.to_csv('./data/ada_prior/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffPrepExperiment(object):\n",
    "    \"\"\"Run auto prep with one set of hyper parameters\"\"\"\n",
    "    def __init__(self, data_dir, dataset, prep_space, model_name, method, similarity_threshold):\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset = dataset\n",
    "        self.prep_space = prep_space\n",
    "        self.model_name = model_name\n",
    "        self.method = method\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "    def run(self, params, verbose=True):        \n",
    "        X, y = load_data_multitask_synthetic_label(self.data_dir, self.dataset, similarity_threshold=self.similarity_threshold)\n",
    "        self.generated_task = {\n",
    "            \"X\": X,\n",
    "            \"y\": y\n",
    "        }\n",
    "        #X, y = load_data_multitask(self.data_dir, self.dataset)\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = build_data(X, y, random_state=params[\"split_seed\"])\n",
    "        \n",
    "        print(\"Dataset shapes: \", X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "        # pre norm for diffprep flex\n",
    "        if self.method == \"diffprep_flex\":\n",
    "            X_train, X_val, X_test = min_max_normalize(X_train, X_val, X_test)\n",
    "            params[\"patience\"] = 10\n",
    "            params[\"num_epochs\"] = 3000\n",
    "\n",
    "        # set random seed\n",
    "        set_random_seed(params)\n",
    "\n",
    "        ## transform pipeline\n",
    "        # define and fit first step\n",
    "        if self.method == \"diffprep_fix\":\n",
    "            prep_pipeline = DiffPrepFixPipeline(self.prep_space, temperature=params[\"temperature\"],\n",
    "                                             use_sample=params[\"sample\"],\n",
    "                                             diff_method=params[\"diff_method\"],\n",
    "                                             init_method=params[\"init_method\"])\n",
    "        elif self.method == \"diffprep_flex\":\n",
    "            prep_pipeline = DiffPrepFlexPipeline(self.prep_space, temperature=params[\"temperature\"],\n",
    "                            use_sample=params[\"sample\"],\n",
    "                            diff_method=params[\"diff_method\"],\n",
    "                            init_method=params[\"init_method\"])\n",
    "        else:\n",
    "            raise Exception(\"Wrong auto prep method\")\n",
    "\n",
    "        prep_pipeline.init_parameters(X_train, X_val, X_test)\n",
    "        print(\"Train size: ({}, {})\".format(X_train.shape[0], prep_pipeline.out_features))\n",
    "\n",
    "        # model\n",
    "        input_dim = prep_pipeline.out_features\n",
    "        output_dim = len(set(y.values.ravel()))\n",
    "\n",
    "        # model = TwoLayerNet(input_dim, output_dim)\n",
    "        set_random_seed(params)\n",
    "        if self.model_name == \"log\":\n",
    "            model = LogisticRegression(input_dim, output_dim)\n",
    "        else:\n",
    "            raise Exception(\"Wrong model\")\n",
    "\n",
    "        model = model.to(params[\"device\"])\n",
    "\n",
    "        # loss\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # optimizer\n",
    "        model_optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=params[\"model_lr\"],\n",
    "            weight_decay=params[\"weight_decay\"],\n",
    "            momentum=params[\"momentum\"]\n",
    "        )\n",
    "        \n",
    "        if params[\"prep_lr\"] is None:\n",
    "            prep_lr = params[\"model_lr\"]\n",
    "        else:\n",
    "            prep_lr = params[\"prep_lr\"]\n",
    "    \n",
    "        prep_pipeline_optimizer = torch.optim.Adam(\n",
    "            prep_pipeline.parameters(),\n",
    "            lr=prep_lr,\n",
    "            betas=(0.5, 0.999),\n",
    "            weight_decay=params[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "        # scheduler\n",
    "        # model_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, patience=patience, factor=0.1, threshold=0.001)\n",
    "        prep_pipeline_scheduler = None\n",
    "        model_scheduler = None\n",
    "\n",
    "        if params[\"logging\"]:\n",
    "            logger = SummaryWriter()\n",
    "        else:\n",
    "            logger = None\n",
    "\n",
    "        diff_prep = DiffPrepSGD(prep_pipeline, model, loss_fn, model_optimizer, prep_pipeline_optimizer,\n",
    "                    model_scheduler, prep_pipeline_scheduler, params, writer=logger)\n",
    "\n",
    "        result, best_model = diff_prep.fit(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "        return result, best_model, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import pearsonr\n",
    "# ada_df = pd.read_csv('./data/ada_prior/data.csv')\n",
    "# ada_df\n",
    "# y_rating = ada_df['label']\n",
    "\n",
    "# X, y = load_data_multitask_synthetic_label(\"data\", \"ada_prior\", 0.7)\n",
    "# #(y == y_rating).mean()\n",
    "# corrs = []\n",
    "# for i in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "#     X, y = load_data_multitask_synthetic_label(\"data\", \"ada_prior\", i)\n",
    "#     corrs.append(pearsonr(y == 'Y', y_rating == 'Y').statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diffprep(data_dir, dataset, result_dir, prep_space, params, model_name, method):\n",
    "    print(\"Dataset:\", dataset, \"Diff Method:\", params[\"diff_method\"], method)\n",
    "\n",
    "    diff_prep_exp = DiffPrepExperiment(data_dir, dataset, prep_space, model_name, method, similarity_threshold=params[\"similarity_threshold\"])\n",
    "    best_result, best_model, best_logger, best_params = grid_search(diff_prep_exp, deepcopy(params))\n",
    "    save_result(best_result, best_model, best_logger, best_params, result_dir, save_model=True)\n",
    "    save_task_label(diff_prep_exp.generated_task['y'], result_dir)\n",
    "    print(\"DiffPrep Finished. val acc:\", best_result[\"best_val_acc\"], \"test acc\", best_result[\"best_test_acc\"])\n",
    "    return best_result, best_model, best_logger, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run diffprep_fix on dataset ada_prior\n",
      "0.0\n",
      "Dataset: ada_prior Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (2738, 13) torch.Size([2738]) (912, 13) torch.Size([912]) (912, 13) torch.Size([912])\n",
      "Train size: (2738, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 800/2000 [02:45<04:08,  4.82it/s, next_eval_time=20s, tr_loss=0.758, val_loss=0.933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.5230263157894737 test acc 0.5076754385964912\n",
      "0.1\n",
      "Dataset: ada_prior Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (2738, 13) torch.Size([2738]) (912, 13) torch.Size([912]) (912, 13) torch.Size([912])\n",
      "Train size: (2738, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 300/2000 [01:01<05:50,  4.85it/s, next_eval_time=20s, tr_loss=3.89, val_loss=6.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.4967105263157895 test acc 0.5296052631578947\n",
      "0.2\n",
      "Dataset: ada_prior Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (2738, 13) torch.Size([2738]) (912, 13) torch.Size([912]) (912, 13) torch.Size([912])\n",
      "Train size: (2738, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [01:22<05:30,  4.84it/s, next_eval_time=20s, tr_loss=1.68, val_loss=1.03]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.5153508771929824 test acc 0.5296052631578947\n",
      "0.30000000000000004\n",
      "Dataset: ada_prior Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (2738, 13) torch.Size([2738]) (912, 13) torch.Size([912]) (912, 13) torch.Size([912])\n",
      "Train size: (2738, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [01:22<05:31,  4.82it/s, next_eval_time=20s, tr_loss=2.75, val_loss=0.891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.5592105263157895 test acc 0.5668859649122807\n",
      "0.4\n",
      "Dataset: ada_prior Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (2738, 13) torch.Size([2738]) (912, 13) torch.Size([912]) (912, 13) torch.Size([912])\n",
      "Train size: (2738, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 1300/2000 [04:28<02:24,  4.84it/s, next_eval_time=21s, tr_loss=0.69, val_loss=0.653] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.6557017543859649 test acc 0.6337719298245614\n",
      "0.5\n",
      "Dataset: ada_prior Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (2738, 13) torch.Size([2738]) (912, 13) torch.Size([912]) (912, 13) torch.Size([912])\n",
      "Train size: (2738, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 800/2000 [02:39<03:58,  5.02it/s, next_eval_time=19s, tr_loss=0.803, val_loss=0.792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.6754385964912281 test acc 0.6578947368421053\n",
      "0.6000000000000001\n",
      "Dataset: ada_prior Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (2738, 13) torch.Size([2738]) (912, 13) torch.Size([912]) (912, 13) torch.Size([912])\n",
      "Train size: (2738, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [01:19<05:16,  5.05it/s, next_eval_time=19s, tr_loss=2.62, val_loss=1.81]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.6732456140350878 test acc 0.6754385964912281\n",
      "0.7000000000000001\n",
      "Dataset: ada_prior Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (2738, 13) torch.Size([2738]) (912, 13) torch.Size([912]) (912, 13) torch.Size([912])\n",
      "Train size: (2738, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 500/2000 [01:38<04:55,  5.08it/s, next_eval_time=19s, tr_loss=2.14, val_loss=3.68]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.7291666666666666 test acc 0.7138157894736842\n",
      "0.8\n",
      "Dataset: ada_prior Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (2738, 13) torch.Size([2738]) (912, 13) torch.Size([912]) (912, 13) torch.Size([912])\n",
      "Train size: (2738, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 1300/2000 [04:16<02:18,  5.07it/s, next_eval_time=19s, tr_loss=0.483, val_loss=0.502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.7730263157894737 test acc 0.7576754385964912\n",
      "0.9\n",
      "Dataset: ada_prior Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (2738, 13) torch.Size([2738]) (912, 13) torch.Size([912]) (912, 13) torch.Size([912])\n",
      "Train size: (2738, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 600/2000 [02:00<04:40,  4.98it/s, next_eval_time=19s, tr_loss=0.685, val_loss=0.615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.7916666666666666 test acc 0.8146929824561403\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from prep_space import space\n",
    "from experiment.baseline_experiment import run_baseline\n",
    "import os\n",
    "\n",
    "# define hyper parameters\n",
    "params = {\n",
    "    \"num_epochs\": 2000,\n",
    "    \"batch_size\": 512,\n",
    "    \"device\": \"cpu\",\n",
    "    #\"model_lr\": [0.1, 0.01, 0.001],\n",
    "    \"model_lr\": 0.01,\n",
    "    \"weight_decay\": 0,\n",
    "    \"model\": 'log',\n",
    "    \"train_seed\": 1,\n",
    "    \"split_seed\": 1,\n",
    "    \"method\": \"diffprep_fix\",\n",
    "    \"save_model\": True,\n",
    "    \"logging\": False,\n",
    "    \"no_crash\": False,\n",
    "    \"patience\": 3,\n",
    "    \"momentum\": 0.9,\n",
    "    \"similarity_threshold\": 0.1,\n",
    "}\n",
    "\n",
    "auto_prep_params = {\n",
    "    \"prep_lr\": None,\n",
    "    \"temperature\": 0.1,\n",
    "    \"grad_clip\": None,\n",
    "    \"pipeline_update_sample_size\": 512,\n",
    "    \"init_method\": \"default\",\n",
    "    \"diff_method\": \"num_diff\",\n",
    "    \"sample\": False\n",
    "}\n",
    "\n",
    "DATADIR = \"data\"\n",
    "\n",
    "params.update(auto_prep_params)\n",
    "\n",
    "datasets = sorted(os.listdir(DATADIR))\n",
    "dataset = \"house_prices\"\n",
    "\n",
    "print(\"Run {} on dataset {}\".format(params[\"method\"], dataset))\n",
    "\n",
    "sims = list(np.arange(0, 1, 0.1))\n",
    "\n",
    "for sim in sims:\n",
    "    print(sim)\n",
    "    params[\"similarity_threshold\"] = sim\n",
    "    #result_dir = utils.makedir([\"result\", params[\"method\"], dataset, f'Rating_ground_truth'])\n",
    "    result_dir = utils.makedir([\"result\", params[\"method\"], dataset, f'label_{round(params[\"similarity_threshold\"], 2)}'])\n",
    "\n",
    "    if params[\"method\"] in [\"diffprep_fix\", \"diffprep_flex\"]:\n",
    "        best_result, best_model, best_logger, best_params = run_diffprep(DATADIR, dataset, result_dir, space, params, params[\"model\"], params[\"method\"])\n",
    "    else:\n",
    "        best_result, best_model, best_logger, best_params = run_baseline(DATADIR, dataset, result_dir, space, params, params[\"model\"], params[\"method\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = utils.makedir([\"result\", params[\"method\"], dataset, \"hoursPerWeek\"])\n",
    "save_result(best_result, best_model, best_logger, best_params, result_dir, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline.0.num_tf_prob_logits torch.Size([5, 5])\n",
      "pipeline.0.cat_tf_prob_logits torch.Size([94, 2])\n",
      "pipeline.1.tf_prob_logits torch.Size([99, 4])\n",
      "pipeline.2.tf_prob_logits torch.Size([99, 10])\n",
      "pipeline.3.tf_prob_logits torch.Size([99, 7])\n"
     ]
    }
   ],
   "source": [
    "# label\n",
    "for md in best_model['prep_pipeline'].keys():\n",
    "    print(md, best_model['prep_pipeline'][md].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline.0.num_tf_prob_logits torch.Size([5, 5])\n",
      "pipeline.0.cat_tf_prob_logits torch.Size([94, 2])\n",
      "pipeline.1.tf_prob_logits torch.Size([99, 4])\n",
      "pipeline.2.tf_prob_logits torch.Size([99, 10])\n",
      "pipeline.3.tf_prob_logits torch.Size([99, 7])\n"
     ]
    }
   ],
   "source": [
    "# hoursPerWeek\n",
    "for md in best_model['prep_pipeline'].keys():\n",
    "    print(md, best_model['prep_pipeline'][md].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps:\n",
    "\n",
    "1. Load the transfer datasets\n",
    "2. Train using diffprep fix on one task\n",
    "3. Create a new diffprep fix pipeline where the pipeline is set to the one we got above, and is frozen\n",
    "4. Train the same model arch on the new task, and see accuracy etc.\n",
    "5. Train the same model arch on the new task with a fresh diffprep pipeline, and see accuracy etc.\n",
    "6. Compared the tau matrices and the operatiosn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffprep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
