{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddharth/miniconda3/envs/diffprep/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "from experiment.experiment_utils import set_random_seed, load_data, build_data, grid_search, makedir, save_result, load_data_multitask, load_data_multitask_synthetic_label, save_task_label\n",
    "from model import LogisticRegression\n",
    "from pipeline.diffprep_flex_pipeline import DiffPrepFlexPipeline\n",
    "from pipeline.diffprep_fix_pipeline import DiffPrepFixPipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from trainer.diffprep_trainer import DiffPrepSGD\n",
    "from utils import SummaryWriter\n",
    "from experiment.experiment_utils import min_max_normalize\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# old_df = pd.read_csv('./data/ada_prior/data_old.csv')\n",
    "# #sns.distplot(old_df['hoursPerWeek'])\n",
    "# #old_df['hoursPerWeek'] = (old_df['hoursPerWeek'] <= 40).map({True: 'Y', False: 'N'})\n",
    "# old_df['label'] = old_df['label'].map({-1: 'N', 1: 'Y'})\n",
    "# old_df.to_csv('./data/ada_prior/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffPrepExperiment(object):\n",
    "    \"\"\"Run auto prep with one set of hyper parameters\"\"\"\n",
    "    def __init__(self, data_dir, dataset, prep_space, model_name, method, similarity_threshold):\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset = dataset\n",
    "        self.prep_space = prep_space\n",
    "        self.model_name = model_name\n",
    "        self.method = method\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "    def run(self, params, verbose=True):        \n",
    "        X, y = load_data_multitask_synthetic_label(self.data_dir, self.dataset, similarity_threshold=self.similarity_threshold)\n",
    "        self.generated_task = {\n",
    "            \"X\": X,\n",
    "            \"y\": y\n",
    "        }\n",
    "        #X, y = load_data_multitask(self.data_dir, self.dataset)\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = build_data(X, y, random_state=params[\"split_seed\"])\n",
    "        \n",
    "        print(\"Dataset shapes: \", X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "        # pre norm for diffprep flex\n",
    "        if self.method == \"diffprep_flex\":\n",
    "            X_train, X_val, X_test = min_max_normalize(X_train, X_val, X_test)\n",
    "            params[\"patience\"] = 10\n",
    "            params[\"num_epochs\"] = 3000\n",
    "\n",
    "        # set random seed\n",
    "        set_random_seed(params)\n",
    "\n",
    "        ## transform pipeline\n",
    "        # define and fit first step\n",
    "        if self.method == \"diffprep_fix\":\n",
    "            prep_pipeline = DiffPrepFixPipeline(self.prep_space, temperature=params[\"temperature\"],\n",
    "                                             use_sample=params[\"sample\"],\n",
    "                                             diff_method=params[\"diff_method\"],\n",
    "                                             init_method=params[\"init_method\"])\n",
    "        elif self.method == \"diffprep_flex\":\n",
    "            prep_pipeline = DiffPrepFlexPipeline(self.prep_space, temperature=params[\"temperature\"],\n",
    "                            use_sample=params[\"sample\"],\n",
    "                            diff_method=params[\"diff_method\"],\n",
    "                            init_method=params[\"init_method\"])\n",
    "        else:\n",
    "            raise Exception(\"Wrong auto prep method\")\n",
    "\n",
    "        prep_pipeline.init_parameters(X_train, X_val, X_test)\n",
    "        print(\"Train size: ({}, {})\".format(X_train.shape[0], prep_pipeline.out_features))\n",
    "\n",
    "        # model\n",
    "        input_dim = prep_pipeline.out_features\n",
    "        output_dim = len(set(y.values.ravel()))\n",
    "\n",
    "        # model = TwoLayerNet(input_dim, output_dim)\n",
    "        set_random_seed(params)\n",
    "        if self.model_name == \"log\":\n",
    "            model = LogisticRegression(input_dim, output_dim)\n",
    "        else:\n",
    "            raise Exception(\"Wrong model\")\n",
    "\n",
    "        model = model.to(params[\"device\"])\n",
    "\n",
    "        # loss\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # optimizer\n",
    "        model_optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=params[\"model_lr\"],\n",
    "            weight_decay=params[\"weight_decay\"],\n",
    "            momentum=params[\"momentum\"]\n",
    "        )\n",
    "        \n",
    "        if params[\"prep_lr\"] is None:\n",
    "            prep_lr = params[\"model_lr\"]\n",
    "        else:\n",
    "            prep_lr = params[\"prep_lr\"]\n",
    "    \n",
    "        prep_pipeline_optimizer = torch.optim.Adam(\n",
    "            prep_pipeline.parameters(),\n",
    "            lr=prep_lr,\n",
    "            betas=(0.5, 0.999),\n",
    "            weight_decay=params[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "        # scheduler\n",
    "        # model_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, patience=patience, factor=0.1, threshold=0.001)\n",
    "        prep_pipeline_scheduler = None\n",
    "        model_scheduler = None\n",
    "\n",
    "        if params[\"logging\"]:\n",
    "            logger = SummaryWriter()\n",
    "        else:\n",
    "            logger = None\n",
    "\n",
    "        diff_prep = DiffPrepSGD(prep_pipeline, model, loss_fn, model_optimizer, prep_pipeline_optimizer,\n",
    "                    model_scheduler, prep_pipeline_scheduler, params, writer=logger)\n",
    "\n",
    "        result, best_model = diff_prep.fit(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "        return result, best_model, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import pearsonr\n",
    "# ada_df = pd.read_csv('./data/ada_prior/data.csv')\n",
    "# ada_df\n",
    "# y_rating = ada_df['label']\n",
    "\n",
    "# X, y = load_data_multitask_synthetic_label(\"data\", \"ada_prior\", 0.7)\n",
    "# #(y == y_rating).mean()\n",
    "# corrs = []\n",
    "# for i in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "#     X, y = load_data_multitask_synthetic_label(\"data\", \"ada_prior\", i)\n",
    "#     corrs.append(pearsonr(y == 'Y', y_rating == 'Y').statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diffprep(data_dir, dataset, result_dir, prep_space, params, model_name, method):\n",
    "    print(\"Dataset:\", dataset, \"Diff Method:\", params[\"diff_method\"], method)\n",
    "\n",
    "    diff_prep_exp = DiffPrepExperiment(data_dir, dataset, prep_space, model_name, method, similarity_threshold=params[\"similarity_threshold\"])\n",
    "    best_result, best_model, best_logger, best_params = grid_search(diff_prep_exp, deepcopy(params))\n",
    "    save_result(best_result, best_model, best_logger, best_params, result_dir, save_model=True)\n",
    "    save_task_label(diff_prep_exp.generated_task['y'], result_dir)\n",
    "    print(\"DiffPrep Finished. val acc:\", best_result[\"best_val_acc\"], \"test acc\", best_result[\"best_test_acc\"])\n",
    "    return best_result, best_model, best_logger, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run diffprep_fix on dataset house_prices\n",
      "0.0\n",
      "Dataset: house_prices Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (876, 80) torch.Size([876]) (292, 80) torch.Size([292]) (292, 80) torch.Size([292])\n",
      "Train size: (876, 295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 300/2000 [01:49<10:19,  2.74it/s, next_eval_time=35s, tr_loss=8.22, val_loss=12.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.4486301369863014 test acc 0.523972602739726\n",
      "0.1\n",
      "Dataset: house_prices Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (876, 80) torch.Size([876]) (292, 80) torch.Size([292]) (292, 80) torch.Size([292])\n",
      "Train size: (876, 295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 300/2000 [01:47<10:10,  2.78it/s, next_eval_time=35s, tr_loss=4.92, val_loss=9]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.4965753424657534 test acc 0.5513698630136986\n",
      "0.2\n",
      "Dataset: house_prices Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (876, 80) torch.Size([876]) (292, 80) torch.Size([292]) (292, 80) torch.Size([292])\n",
      "Train size: (876, 295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [02:26<09:44,  2.74it/s, next_eval_time=35s, tr_loss=11.6, val_loss=18.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.5753424657534246 test acc 0.5034246575342466\n",
      "0.30000000000000004\n",
      "Dataset: house_prices Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (876, 80) torch.Size([876]) (292, 80) torch.Size([292]) (292, 80) torch.Size([292])\n",
      "Train size: (876, 295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [02:26<09:44,  2.74it/s, next_eval_time=36s, tr_loss=7, val_loss=11.7]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.6061643835616438 test acc 0.5547945205479452\n",
      "0.4\n",
      "Dataset: house_prices Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (876, 80) torch.Size([876]) (292, 80) torch.Size([292]) (292, 80) torch.Size([292])\n",
      "Train size: (876, 295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 500/2000 [03:04<09:13,  2.71it/s, next_eval_time=37s, tr_loss=2.1, val_loss=2.07]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.6027397260273972 test acc 0.6267123287671232\n",
      "0.5\n",
      "Dataset: house_prices Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (876, 80) torch.Size([876]) (292, 80) torch.Size([292]) (292, 80) torch.Size([292])\n",
      "Train size: (876, 295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [02:27<09:50,  2.71it/s, next_eval_time=36s, tr_loss=1.92, val_loss=2.4]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.6438356164383562 test acc 0.5993150684931506\n",
      "0.6000000000000001\n",
      "Dataset: house_prices Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (876, 80) torch.Size([876]) (292, 80) torch.Size([292]) (292, 80) torch.Size([292])\n",
      "Train size: (876, 295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [02:26<09:46,  2.73it/s, next_eval_time=36s, tr_loss=5.63, val_loss=11.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.6301369863013698 test acc 0.678082191780822\n",
      "0.7000000000000001\n",
      "Dataset: house_prices Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (876, 80) torch.Size([876]) (292, 80) torch.Size([292]) (292, 80) torch.Size([292])\n",
      "Train size: (876, 295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [02:26<09:46,  2.73it/s, next_eval_time=37s, tr_loss=3.85, val_loss=5.78]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.75 test acc 0.7773972602739726\n",
      "0.8\n",
      "Dataset: house_prices Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (876, 80) torch.Size([876]) (292, 80) torch.Size([292]) (292, 80) torch.Size([292])\n",
      "Train size: (876, 295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [02:26<09:44,  2.74it/s, next_eval_time=35s, tr_loss=1.45, val_loss=2.39]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.8082191780821918 test acc 0.815068493150685\n",
      "0.9\n",
      "Dataset: house_prices Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (876, 80) torch.Size([876]) (292, 80) torch.Size([292]) (292, 80) torch.Size([292])\n",
      "Train size: (876, 295)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 500/2000 [03:02<09:08,  2.74it/s, next_eval_time=36s, tr_loss=0.684, val_loss=0.671]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.886986301369863 test acc 0.8698630136986302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from prep_space import space\n",
    "from experiment.baseline_experiment import run_baseline\n",
    "import os\n",
    "\n",
    "# define hyper parameters\n",
    "params = {\n",
    "    \"num_epochs\": 2000,\n",
    "    \"batch_size\": 512,\n",
    "    \"device\": \"cpu\",\n",
    "    #\"model_lr\": [0.1, 0.01, 0.001],\n",
    "    \"model_lr\": 0.01,\n",
    "    \"weight_decay\": 0,\n",
    "    \"model\": 'log',\n",
    "    \"train_seed\": 1,\n",
    "    \"split_seed\": 1,\n",
    "    \"method\": \"diffprep_fix\",\n",
    "    \"save_model\": True,\n",
    "    \"logging\": False,\n",
    "    \"no_crash\": False,\n",
    "    \"patience\": 3,\n",
    "    \"momentum\": 0.9,\n",
    "    \"similarity_threshold\": 0.1,\n",
    "}\n",
    "\n",
    "auto_prep_params = {\n",
    "    \"prep_lr\": None,\n",
    "    \"temperature\": 0.1,\n",
    "    \"grad_clip\": None,\n",
    "    \"pipeline_update_sample_size\": 512,\n",
    "    \"init_method\": \"default\",\n",
    "    \"diff_method\": \"num_diff\",\n",
    "    \"sample\": False\n",
    "}\n",
    "\n",
    "DATADIR = \"data\"\n",
    "\n",
    "params.update(auto_prep_params)\n",
    "\n",
    "datasets = sorted(os.listdir(DATADIR))\n",
    "dataset = \"house_prices\"\n",
    "\n",
    "print(\"Run {} on dataset {}\".format(params[\"method\"], dataset))\n",
    "\n",
    "sims = list(np.arange(0, 1, 0.1))\n",
    "\n",
    "for sim in sims:\n",
    "    print(sim)\n",
    "    params[\"similarity_threshold\"] = sim\n",
    "    #result_dir = utils.makedir([\"result\", params[\"method\"], dataset, f'Rating_ground_truth'])\n",
    "    result_dir = utils.makedir([\"result\", params[\"method\"], dataset, f'label_{round(params[\"similarity_threshold\"], 2)}'])\n",
    "\n",
    "    if params[\"method\"] in [\"diffprep_fix\", \"diffprep_flex\"]:\n",
    "        best_result, best_model, best_logger, best_params = run_diffprep(DATADIR, dataset, result_dir, space, params, params[\"model\"], params[\"method\"])\n",
    "    else:\n",
    "        best_result, best_model, best_logger, best_params = run_baseline(DATADIR, dataset, result_dir, space, params, params[\"model\"], params[\"method\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = utils.makedir([\"result\", params[\"method\"], dataset, \"hoursPerWeek\"])\n",
    "save_result(best_result, best_model, best_logger, best_params, result_dir, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline.0.num_tf_prob_logits torch.Size([5, 5])\n",
      "pipeline.0.cat_tf_prob_logits torch.Size([94, 2])\n",
      "pipeline.1.tf_prob_logits torch.Size([99, 4])\n",
      "pipeline.2.tf_prob_logits torch.Size([99, 10])\n",
      "pipeline.3.tf_prob_logits torch.Size([99, 7])\n"
     ]
    }
   ],
   "source": [
    "# label\n",
    "for md in best_model['prep_pipeline'].keys():\n",
    "    print(md, best_model['prep_pipeline'][md].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline.0.num_tf_prob_logits torch.Size([5, 5])\n",
      "pipeline.0.cat_tf_prob_logits torch.Size([94, 2])\n",
      "pipeline.1.tf_prob_logits torch.Size([99, 4])\n",
      "pipeline.2.tf_prob_logits torch.Size([99, 10])\n",
      "pipeline.3.tf_prob_logits torch.Size([99, 7])\n"
     ]
    }
   ],
   "source": [
    "# hoursPerWeek\n",
    "for md in best_model['prep_pipeline'].keys():\n",
    "    print(md, best_model['prep_pipeline'][md].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps:\n",
    "\n",
    "1. Load the transfer datasets\n",
    "2. Train using diffprep fix on one task\n",
    "3. Create a new diffprep fix pipeline where the pipeline is set to the one we got above, and is frozen\n",
    "4. Train the same model arch on the new task, and see accuracy etc.\n",
    "5. Train the same model arch on the new task with a fresh diffprep pipeline, and see accuracy etc.\n",
    "6. Compared the tau matrices and the operatiosn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffprep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
