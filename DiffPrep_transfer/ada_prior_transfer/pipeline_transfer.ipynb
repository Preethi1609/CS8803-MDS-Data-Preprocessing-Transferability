{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddharth/miniconda3/envs/diffprep/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "from experiment.experiment_utils import set_random_seed, load_data, build_data, grid_search, makedir, save_result, load_data_multitask\n",
    "from model import LogisticRegression\n",
    "from pipeline.diffprep_flex_pipeline import DiffPrepFlexPipeline\n",
    "from pipeline.diffprep_fix_pipeline import DiffPrepFixPipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from trainer.diffprep_trainer import DiffPrepSGD\n",
    "from utils import SummaryWriter\n",
    "from experiment.experiment_utils import min_max_normalize\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prep_pipeline(path, prep_space, params, data_dir, dataset):\n",
    "\n",
    "    X, y = load_data_multitask(data_dir, dataset)\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = build_data(X, y, random_state=params[\"split_seed\"])\n",
    "\n",
    "    prep_pipeline = DiffPrepFixPipeline(prep_space, temperature=params[\"temperature\"],\n",
    "                                use_sample=False,\n",
    "                                diff_method=params[\"diff_method\"],\n",
    "                                init_method=params[\"init_method\"])\n",
    "    prep_pipeline.init_parameters(X_train, X_val, X_test)\n",
    "    prep_pipeline.load_state_dict(torch.load(path))\n",
    "    prep_pipeline.is_fitted = True\n",
    "    prep_pipeline.fit(X_train)\n",
    "    #prep_pipeline.eval()\n",
    "\n",
    "    return prep_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffPrepExperiment(object):\n",
    "    \"\"\"Run auto prep with one set of hyper parameters\"\"\"\n",
    "    def __init__(self, data_dir, dataset, prep_space, model_name, method, fixed_pipeline_path=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset = dataset\n",
    "        self.prep_space = prep_space\n",
    "        self.model_name = model_name\n",
    "        self.method = method\n",
    "        self.fixed_pipeline_path = fixed_pipeline_path\n",
    "\n",
    "    def run(self, params, verbose=True):        \n",
    "        X, y = load_data_multitask(self.data_dir, self.dataset)\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = build_data(X, y, random_state=params[\"split_seed\"])\n",
    "        \n",
    "        print(\"Dataset shapes: \", X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "        # pre norm for diffprep flex\n",
    "        if self.method == \"diffprep_flex\":\n",
    "            X_train, X_val, X_test = min_max_normalize(X_train, X_val, X_test)\n",
    "            params[\"patience\"] = 10\n",
    "            params[\"num_epochs\"] = 3000\n",
    "\n",
    "        # set random seed\n",
    "        set_random_seed(params)\n",
    "        ## transform pipeline\n",
    "        # define and fit first step\n",
    "        if self.fixed_pipeline_path:\n",
    "            #prep_pipeline = self.fixed_pipeline\n",
    "            prep_pipeline = DiffPrepFixPipeline(self.prep_space, temperature=params[\"temperature\"],\n",
    "                                use_sample=False,\n",
    "                                diff_method=params[\"diff_method\"],\n",
    "                                init_method=params[\"init_method\"])\n",
    "            prep_pipeline.init_parameters(X_train, X_val, X_test)\n",
    "            prep_pipeline.load_state_dict(torch.load(self.fixed_pipeline_path))\n",
    "            prep_pipeline.fit(X_train)\n",
    "            prep_pipeline.is_fitted = True\n",
    "\n",
    "        elif self.method == \"diffprep_fix\":\n",
    "            prep_pipeline = DiffPrepFixPipeline(self.prep_space, temperature=params[\"temperature\"],\n",
    "                                             use_sample=params[\"sample\"],\n",
    "                                             diff_method=params[\"diff_method\"],\n",
    "                                             init_method=params[\"init_method\"])\n",
    "            prep_pipeline.init_parameters(X_train, X_val, X_test)\n",
    "        elif self.method == \"diffprep_flex\":\n",
    "            prep_pipeline = DiffPrepFlexPipeline(self.prep_space, temperature=params[\"temperature\"],\n",
    "                            use_sample=params[\"sample\"],\n",
    "                            diff_method=params[\"diff_method\"],\n",
    "                            init_method=params[\"init_method\"])\n",
    "            prep_pipeline.init_parameters(X_train, X_val, X_test)\n",
    "        else:\n",
    "            raise Exception(\"Wrong auto prep method\")\n",
    "\n",
    "        #prep_pipeline.init_parameters(X_train, X_val, X_test)\n",
    "        print(\"Train size: ({}, {})\".format(X_train.shape[0], prep_pipeline.out_features))\n",
    "\n",
    "        # model\n",
    "        input_dim = prep_pipeline.out_features\n",
    "        output_dim = len(set(y.values.ravel()))\n",
    "\n",
    "        # model = TwoLayerNet(input_dim, output_dim)\n",
    "        set_random_seed(params)\n",
    "        if self.model_name == \"log\":\n",
    "            model = LogisticRegression(input_dim, output_dim)\n",
    "        else:\n",
    "            raise Exception(\"Wrong model\")\n",
    "\n",
    "        model = model.to(params[\"device\"])\n",
    "\n",
    "        # loss\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # optimizer\n",
    "        model_optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=params[\"model_lr\"],\n",
    "            weight_decay=params[\"weight_decay\"],\n",
    "            momentum=params[\"momentum\"]\n",
    "        )\n",
    "        \n",
    "        if params[\"prep_lr\"] is None:\n",
    "            prep_lr = params[\"model_lr\"]\n",
    "        else:\n",
    "            prep_lr = params[\"prep_lr\"]\n",
    "    \n",
    "        prep_pipeline_optimizer = None\n",
    "        # torch.optim.Adam(\n",
    "        #     prep_pipeline.parameters(),\n",
    "        #     lr=prep_lr,\n",
    "        #     betas=(0.5, 0.999),\n",
    "        #     weight_decay=params[\"weight_decay\"]\n",
    "        # )\n",
    "\n",
    "        # scheduler\n",
    "        # model_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, patience=patience, factor=0.1, threshold=0.001)\n",
    "        prep_pipeline_scheduler = None\n",
    "        model_scheduler = None\n",
    "\n",
    "        if params[\"logging\"]:\n",
    "            logger = SummaryWriter()\n",
    "        else:\n",
    "            logger = None\n",
    "\n",
    "        diff_prep = DiffPrepSGD(prep_pipeline, model, loss_fn, model_optimizer, prep_pipeline_optimizer,\n",
    "                    model_scheduler, prep_pipeline_scheduler, params, writer=logger, train_pipeline=False)\n",
    "\n",
    "        result, best_model = diff_prep.fit(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "        return result, best_model, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diffprep(data_dir, dataset, result_dir, prep_space, params, model_name, method, prep_pipeline_price_path):\n",
    "    print(\"Dataset:\", dataset, \"Diff Method:\", params[\"diff_method\"], method)\n",
    "\n",
    "    sample = \"sample\" if params[\"sample\"] else \"nosample\"\n",
    "    diff_prep_exp = DiffPrepExperiment(data_dir, dataset, prep_space, model_name, method, fixed_pipeline_path=prep_pipeline_price_path)\n",
    "    best_result, best_model, best_logger, best_params = grid_search(diff_prep_exp, deepcopy(params))\n",
    "    save_result(best_result, best_model, best_logger, best_params, result_dir, save_model=True)\n",
    "    print(\"DiffPrep Finished. val acc:\", best_result[\"best_val_acc\"], \"test acc\", best_result[\"best_test_acc\"])\n",
    "    return best_result, best_model, best_logger, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run diffprep_fix on dataset ada_prior\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from prep_space import space\n",
    "from experiment.baseline_experiment import run_baseline\n",
    "import os\n",
    "\n",
    "# define hyper parameters\n",
    "params = {\n",
    "    \"num_epochs\": 2000,\n",
    "    \"batch_size\": 512,\n",
    "    \"device\": \"cpu\",\n",
    "    #\"model_lr\": [0.1, 0.01, 0.001],\n",
    "    \"model_lr\": 0.01,\n",
    "    \"weight_decay\": 0,\n",
    "    \"model\": 'log',\n",
    "    \"train_seed\": 1,\n",
    "    \"split_seed\": 1,\n",
    "    \"method\": \"diffprep_fix\",\n",
    "    \"save_model\": True,\n",
    "    \"logging\": False,\n",
    "    \"no_crash\": False,\n",
    "    \"patience\": 3,\n",
    "    \"momentum\": 0.9\n",
    "}\n",
    "\n",
    "auto_prep_params = {\n",
    "    \"prep_lr\": None,\n",
    "    \"temperature\": 0.1,\n",
    "    \"grad_clip\": None,\n",
    "    \"pipeline_update_sample_size\": 512,\n",
    "    \"init_method\": \"default\",\n",
    "    \"diff_method\": \"num_diff\",\n",
    "    \"sample\": False\n",
    "}\n",
    "\n",
    "DATADIR = \"data\"\n",
    "\n",
    "params.update(auto_prep_params)\n",
    "\n",
    "datasets = sorted(os.listdir(DATADIR))\n",
    "dataset = \"ada_prior\"\n",
    "\n",
    "print(\"Run {} on dataset {}\".format(params[\"method\"], dataset))\n",
    "\n",
    "result_dir = utils.makedir([\"result\", params[\"method\"], dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ada_prior Diff Method: num_diff diffprep_fix\n",
      "Model lr 0.01\n",
      "Dataset shapes:  (2738, 13) torch.Size([2738]) (912, 13) torch.Size([912]) (912, 13) torch.Size([912])\n",
      "Train size: (2738, 99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [00:40<02:40,  9.97it/s, next_eval_time=9s, tr_loss=9.77, val_loss=20.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffPrep Finished. val acc: 0.8212719298245614 test acc 0.8442982456140351\n"
     ]
    }
   ],
   "source": [
    "prep_pipeline_hpw_path = './result/diffprep_fix/ada_prior/hoursPerWeek/prep_pipeline.pth'\n",
    "\n",
    "if params[\"method\"] in [\"diffprep_fix\", \"diffprep_flex\"]:\n",
    "    best_result, best_model, best_logger, best_params = run_diffprep(DATADIR, dataset, result_dir, space, params, params[\"model\"], params[\"method\"], prep_pipeline_hpw_path)\n",
    "else:\n",
    "    best_result, best_model, best_logger, best_params = run_baseline(DATADIR, dataset, result_dir, space, params, params[\"model\"], params[\"method\"], prep_pipeline_hpw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffprep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
